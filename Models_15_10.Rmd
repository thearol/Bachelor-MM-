---
title: "Models"
author: "Thea Rolskov Sloth"
date: "15/10/2019"
output: html_document
---

#Getting packages
```{r loading packages, include = FALSE}
library(pacman)
p_load(brms, tidyverse, tidybayes, ggplot2, LaplacesDemon, rethinking, tidyr, reshape2, tibble, plotly, jpeg, tm, ggrepel)
```

#Setting directory
```{r}
setwd("~/Bachelor-MM-/") #TRS
```

#Loading data
```{r}
data_ib <- read.csv("implicit_bias/data_ib.csv")[,2:12]
data_eb <- read.csv("explicit_bias/eb_results.csv")[, 2:6]
data_db <- read.csv("decision_bias/disagree_sensitivity_db_long.csv")[, 1:34]
```


#IMPLICIT BIAS, MODELS

```{r implicit model, condition}

#to figure out which prior we need to make
get_prior(rt_log ~ condition + (condition|ID), data = data_ib)

#Defining a prior 
prior_ib1 = c(prior(normal(-0.4, 0.2), class = "Intercept"), 
          prior(normal(0, 0.2), class = "b"), 
          prior(normal(0, 0.2), class = "sigma"),
          prior(normal(0, 0.1), class = "sd"))



#Implicit model
model_ib1_prior <- brm(rt_log ~ condition + (condition|ID),
                    data = data_ib,
                    family = gaussian(),
                    prior = prior_ib1,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)


pp_check(model_ib1_prior, nsamples = 100) #Doing a predictive prior check to test priors


model_ib1 <- brm(rt_log ~ condition + (condition|ID),
                    data = data_ib,
                    family = gaussian(),
                    prior = prior_ib1,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4)

summary(model_ib1)


#plotting
plot(model_ib1)

marginal_effects(model_ib1)
```



##Difference between gender and condition, interaction
```{r model, condition + gender}
#to figure out which prior I need to make 
get_prior(rt ~  condition*gender + (condition|ID), data = data_ib)

#Defining a prior OBS!
prior_ib2 = c(prior(normal(-0.4, 0.2), class = "Intercept"),
              prior(normal(0, 0.2), class = "b"),
              prior(normal(0, 0.2), class = "sigma"),
              prior(normal(0, 0.1), class = "sd"))
              



#Implicit model
model_ib2_prior <- brm(rt_log ~ condition*gender + (condition|ID),
                    data = data_ib,
                    family = gaussian(),
                    prior = prior_ib2,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)


pp_check(model_ib2_prior, nsamples = 100) #Doing a predictive prior check to test priors


model_ib2 <- brm(rt_log ~ condition*gender + (condition|ID),
                    data = data_ib,
                    family = gaussian(),
                    prior = prior_ib2,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4)



summary(model_ib2) 

#plotting
marginal_effects(model_ib2)

plot(model_ib2)

```



##Implicit bias, Comparing models
```{r waic, implicit}
waic(model_ib1, model_ib2)

waic1 <- waic(model_ib1)
waic2 <- waic(model_ib2)

loo_compare(waic1, waic2)
```


##Implicit bias, hypothesis testing
```{r}
hypothesis(model_ib1, "condition > 0")
hypothesis(model_ib2, "condition = condition + condition:genderMale")
```

#EXPLICIT BIAS

Lav en model, der viser om mænd er mere sexistiske. 

Men det informerer ikkke spørgsmålet, om der et kønsbias synderligt, det den kun er rettet imod at beskrive kønnenes indstilling til kvinder?  

##Overall_score

```{r overall score, models}
#to figure out which prior we need to make
get_prior(overall_score ~ gender + (1|ID), data = data_eb)

#Defining a prior, mean set to 2.5 because best conceptual meaning
prior_eb1.1 = c(prior(normal(2.5, 0.5), class = "Intercept"), 
          prior(normal(0, 0.5), class = "b"), 
          prior(normal(0, 0.5), class = "sigma"),
          prior(normal(0, 0.1), class = "sd"))

#try and make a prior with mean set to 2 as it might make the model better
prior_eb1.2 = c(prior(normal(2, 0.5), class = "Intercept"), 
          prior(normal(0, 0.5), class = "b"), 
          prior(normal(0, 0.5), class = "sigma"),
          prior(normal(0, 0.1), class = "sd"))



#Implicit models - check of both priors
model_eb1.1_prior <- brm(overall_score ~ gender + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb1.1,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)

model_eb1.2_prior <- brm(overall_score ~ gender + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb1.2,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)

#prior check for both priors
pp_check(model_eb1.1_prior, nsamples = 100) #Doing a predictive prior check to test priors

pp_check(model_eb1.2_prior, nsamples = 100)

#models with both priors 
model_eb1.1 <- brm(overall_score ~ gender + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb1.1,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4, 
                    control = list(adapt_delta = 0.9))

model_eb1.2 <- brm(overall_score ~ gender + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb1.2,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4, 
                    control = list(adapt_delta = 0.9)) 

waic(model_eb1.1, model_eb1.2)

summary(model_eb1.2)

marginal_effects(model_eb1.2)

summary(model_eb1.1)

marginal_effects(model_eb1.1)

plot(model_eb1.1)

#we will go with model eb1.1
```

##Benevolence predicted from gender and hostile score

```{r}
get_prior(benevolent_score ~ gender*hostile_score + (1|ID), data = data_eb)

#Defining a prior, mean set to 2.5 because best conceptual meaning
prior_eb2 = c(prior(normal(2.5, 0.5), class = "Intercept"), 
          prior(normal(0, 0.5), class = "b"), 
          prior(normal(0, 0.5), class = "sigma"),
          prior(normal(0, 0.1), class = "sd"))

model_eb2_prior <- brm(benevolent_score ~ gender*hostile_score + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb2,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)

#prior check for both priors
pp_check(model_eb2_prior, nsamples = 100) #Doing a predictive prior check to test priors

#model
model_eb2 <- brm(benevolent_score ~ gender*hostile_score + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb2,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4, 
                    control = list(adapt_delta = 0.9))

summary(model_eb2)

#Plotting
marginal_effects(model_eb2)

plot(model_eb2)
```


##Hostility predicted from gender and benevolent score

```{r}
get_prior(hostile_score ~ gender*benevolent_score + (1|ID), data = data_eb)

#Defining a prior, mean set to 2.5 because best conceptual meaning
prior_eb3 = c(prior(normal(2.5, 0.5), class = "Intercept"), 
          prior(normal(0, 0.5), class = "b"), 
          prior(normal(0, 0.5), class = "sigma"),
          prior(normal(0, 0.1), class = "sd"))

model_eb3_prior <- brm(hostile_score ~ gender*benevolent_score + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb3,
                    sample_prior = "only",
                    cores = 2,
                    chains = 2)

#prior check for both priors
pp_check(model_eb3_prior, nsamples = 100) #Doing a predictive prior check to test priors

#model
model_eb3 <- brm(hostile_score ~ gender*benevolent_score + (1|ID),
                    data = data_eb,
                    family = gaussian(),
                    prior = prior_eb3,
                    sample_prior = TRUE,
                    iter = 5000,
                    warmup = 2000,
                    cores = 4,
                    chains = 4, 
                    control = list(adapt_delta = 0.9))

summary(model_eb3)

#Plotting
marginal_effects(model_eb3)

plot(model_eb3)
```

##Hypothesis testing
```{r}
#Model 1
#There is no difference between females and males in overall sexism score
hypothesis(model_eb1.1, "genderMale = 0")


#Model 2
#There is no difference in benevolence score across gender
hypothesis(model_eb2, "genderMale = 0")

#There is no difference in benevolent score across gender as an effect of hostile score
hypothesis(model_eb2, "genderMale:hostile_score = 0")

#There is a correlation between benevolent score and hostile score
hypothesis(model_eb2, "hostile_score > 0")


#Model 3
#There is no difference in hostile score across gender
hypothesis(model_eb3, "genderMale = 0")

#There is no difference in hostile score across gender as an effect of benevolent score
hypothesis(model_eb3, "genderMale:benevolent_score = 0")

#There is a correlation between hostile score and benevolent score
hypothesis(model_eb3, "benevolent_score > 0")

```


#DECISION BIAS

Lav modeller magen til dem fra sidste semester

##Are men better than women? (ooooops, we did it again... look at this later!!!! (if we exclude skill in the model))
```{r is men better than women regression, include = FALSE}

#Defining priors
prior_skill_gender_db <- c(
  prior(normal(0, 0.2),class="Intercept"),
  prior(normal(0,0.1),class="b"),
  prior(normal(0,0.2), class= "sd", coef = "diff", group = "ID"),
  prior(normal(0,0.1), class= "sd", coef = "Intercept", group = "ID")
)

# Model w skill difference
skill_gender_db <- brm(
  answer ~ diff + diff:Gender + (1+diff|ID), 
  data = data_db,
  prior = prior_skill_gender_db,
  family = "bernoulli",
  sample_prior=T,
  seed = 123, # Adding a seed makes results reproducible.
  cores=2,
  chains=2
) 

summary(skill_mf)

prior_mf_correct <- c(
  prior(normal(0, 0.2),class="Intercept"),
  prior(normal(0,0.1),class="b")
  )


correct_mf <- brm(
  correct ~ Gender, 
  data = pool_long,
  prior = prior_mf_correct,
  family = "bernoulli",
  sample_prior=T,
  seed = 123, # Adding a seed makes results reproducible.
  cores=2,
  chains=2
) 

summary(correct_mf)


male <- subset(pool_long, Gender == "Male")

female <- subset(pool_long, Gender == "Female")

table(male$correct)

table(female$correct)

```
 






##Analysis from last year (excluding skill difference)

```{r}

#Defining priors
prior_db1 <- c(
  prior(normal(0,.2),class="b"),
  prior(normal(0,.1),class="sd")
)

#prior predictive check
model_db1_prior <- brm(leader_behavior ~ 0 + Leader_gender : Follower_gender + (0 + skill_dif:Follower_gender | SubjectID), 
                       prior = prior_db1, 
                       data = data_db, 
                       sample_prior = "only",
                       iter = 4000, 
                       family = "bernoulli", 
                       chains = 2, 
                       cores = 2)

pp_check(model_db1_prior, nsamples = 100)


model_db1 <- brm(
  leader_behavior ~ 0 + Leader_gender : Follower_gender + ( 0 + skill_dif : Follower_gender | SubjectID),
  data = data_db,
  prior = prior_db1,
  sample_prior=T,
  family = "bernoulli", #As we had a binary outcome, we set this to "bernoulli"
  seed = 123, # Adding a seed makes results reproducible.
  chains=4,
  cores=4
  #control = list(adapt_delta = 0.9) 
) 
summary(model_db1)
```


# H1: there is a leader effect: male leaders tend to surrender less than female leaders
```{r H1: there is a leader effect: male leaders tend to surrender less than female leaders}
#H1: In general male leader tend to surrender less than female leaders

hypothesis(model_db1, "(Leader_genderMale:Follower_genderMale + Leader_genderMale:Follower_genderFemale)/2 < (Leader_genderFemale:Follower_genderMale + Leader_genderFemale:Follower_genderMale)/2")



```



#H2: there is a follower effect: leaders tend to surrender more to men than to women
```{r H2: there is a follower effect: leaders tend to surrender more to men than to women }

# H2: There is a follower effect: leaders tend to surrender more to men than to women
hypothesis(model_db1, "(Leader_genderMale:Follower_genderMale + Leader_genderFemale:Follower_genderMale)/2 > (Leader_genderMale:Follower_genderFemale + Leader_genderFemale:Follower_genderFemale)/2")


```



```{r H3: there is an interaction, males are more discriminative as to their followers gender than females are }
# H3: There is an interaction: males are more discriminative as to their followers gender than females are

hypothesis(model_db1, "(Leader_genderMale:Follower_genderMale - Leader_genderMale:Follower_genderFemale) > (Leader_genderFemale:Follower_genderMale - Leader_genderFemale:Follower_genderFemale)")


```

#PLOTTING

```{r prepare predictions}
8+8
#create newdata to make predictions from 
nd <- expand.grid(tibble(
         Follower_gender=factor(0:1) %>% rep(., times = 10),
         Leader_gender = factor(0:1) %>% rep(., times = 10),
         SubjectID = NA))

#predict probabilities for surrender, use fitted to get upper and lower quantile measures in probabilities

pred <-
  predict(m_0, newdata = nd, re_formula = ~ (0 + Follower_gender | Subject)) %>%  # we can use the same nd data from last time
  as_tibble() %>%
  bind_cols(nd)

pred$Leader_gender <- as.character(pred$Leader_gender)
pred$Follower_gender <- as.character(pred$Follower_gender)


pred$Follower_gender[pred$Follower_gender == 0] <- "Male"
pred$Follower_gender[pred$Follower_gender == 1] <- "Female"
pred$Leader_gender[pred$Leader_gender == 0] <- "Male"
pred$Leader_gender[pred$Leader_gender == 1] <- "Female"



#GENDER BIAS

Lav modeller, der kombinerer data fra de tre forskellige dele. 






